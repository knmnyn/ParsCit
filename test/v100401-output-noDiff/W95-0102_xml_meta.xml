<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="100401">
<algorithm name="ParsHed" version="100401">
<variant no="0" confidence="0.845606">
<title confidence="0.993933">Lexical Heads, Phrase Structure and the Induction of Grammar</title>
<author confidence="0.994982">Carl de_Marcken</author>
<affiliation confidence="0.999948">MIT Artificial Intelligence Laboratory</affiliation>
<address confidence="0.982963">NE43-804 545 Technology Square Cambridge, MA, 02139, USA</address>
<email confidence="0.997916">cgdemarcaai.mit.edu</email>
<abstract confidence="0.984180428571429">Summary Acquiring linguistically plausible phrase-structure grammars from ordinary text has proven difficult for standard induction techniques, and researchers have turned to supervised training from bracketed corpora. We examine why previous approaches have failed to acquire desired grammars, concentrating our analysis on the inside-outside algorithm (Baker, 1979), and propose that with a representation of phrase structure centered on head relations such supervision may not be necessary</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="100401">
<citationList>
<citation valid="true">
<authors>
<author>James K Baker</author>
</authors>
<title>Trainable grammars for speech recognition</title>
<date>1979</date>
<booktitle>In Proceedings of the 97th Meeting of the Acoustical Society of America</booktitle>
<pages>547--550</pages>
<contexts>
<context position="561" citStr="Baker, 1979">hers have turned to supervised training from bracketed corpora. We examine why previous approaches have failed to acquire desired grammars, concentrating our analysis on the inside-outside algorithm (Baker, 1979), and propose that with a representation of phrase structure centered on head relations such supervision may not be necessary. 1. INTRODUCTION Researchers investigating the acquisition of phrase-struc</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>James K. Baker. 1979. Trainable grammars for speech recognition. In Proceedings of the 97th Meeting of the Acoustical Society of America, pages 547-550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Brent</author>
</authors>
<title>Minimal generative explanations: A middle ground between neurons and triggers</title>
<date>1993</date>
<booktitle>In Proc. of the 15th Annual Meeting of the Cognitive Science Society</booktitle>
<pages>28--36</pages>
<contexts>
<context position="8875" citStr="Brent, 1993">on the acquisition of syntax, similar or identical statistical models to those discussed here have been used to acquiring words and morphemes from sequences of characters (Olivier, 1968; Wolff, 1982; Brent, 1993; Cartwright and Brent, 1994) and syllables from phonemes (Ellison, 1992), among other language applications. 16 explore ways of fixing them. Let us look again at (A), reproduced below, and center dis</context>
</contexts>
<marker>Brent, 1993</marker>
<rawString>Michael Brent. 1993. Minimal generative explanations: A middle ground between neurons and triggers. In Proc. of the 15th Annual Meeting of the Cognitive Science Society, pages 28-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Automatic grammar induction and parsing free text: A transformation based approach</title>
<date>1993</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop</booktitle>
<contexts>
<context position="1379" citStr="Brill, 1993"> Treebank (Marcus, 1991) to train the learner: (Pereira and Schabes, 1992) demonstrate that the inside-outside algorithm can learn grammars effectively given such constraint; from a bracketed corpus (Brill, 1993) successfully learns rules that iteratively transform a default phrase-structure into a better one for a particular sentence. The necessity of bracketed corpora for training is grating to our sensibil</context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>Eric Brill. 1993. Automatic grammar induction and parsing free text: A transformation based approach. In Proceedings of the DARPA Speech and Natural Language Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>Nick Waegner</author>
</authors>
<title>Robust stochastic parsing using the inside-outside algorithm</title>
<date>1992</date>
<booktitle>In Proc. of the AAAI Workshop on Probabilistic-Based Natural Language Processing Techniques</booktitle>
<pages>39--52</pages>
<contexts>
<context position="13790" citStr="Briscoe and Waegner, 1992">exhaustive set of stochastic context-free rules of a certain form, and estimate probabilities for these rules from a test corpus. This is the same general procedure as used by (Lan i and Young, 1990; Briscoe and Waegner, 1992; Pereira and Schabes, 1992) and others. For parts-of-speech Y and Z, the rules we include in our base grammar are S ZP	ZP ZP YP ZP YP ZP ZP Z YP ZP YP Z ZP Z where S is the root nonterminal. As is us</context>
</contexts>
<marker>Briscoe, Waegner, 1992</marker>
<rawString>Ted Briscoe and Nick Waegner. 1992. Robust stochastic parsing using the inside-outside algorithm. In Proc. of the AAAI Workshop on Probabilistic-Based Natural Language Processing Techniques, pages 39-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Andrew Cartwright</author>
<author>Michael R Brent</author>
</authors>
<title>Segmenting speech without a lexicon: Evidence for a bootstrapping model of lexical acquisition</title>
<date>1994</date>
<booktitle>In Proc. of the 16th Annual Meeting of the Cognitive Science Society</booktitle>
<location>Hillsdale, New Jersey</location>
<contexts>
<context position="8904" citStr="Cartwright and Brent, 1994">ition of syntax, similar or identical statistical models to those discussed here have been used to acquiring words and morphemes from sequences of characters (Olivier, 1968; Wolff, 1982; Brent, 1993; Cartwright and Brent, 1994) and syllables from phonemes (Ellison, 1992), among other language applications. 16 explore ways of fixing them. Let us look again at (A), reproduced below, and center discussion on an extended stocha</context>
</contexts>
<marker>Cartwright, Brent, 1994</marker>
<rawString>Timothy Andrew Cartwright and Michael R. Brent. 1994. Segmenting speech without a lexicon: Evidence for a bootstrapping model of lexical acquisition. In Proc. of the 16th Annual Meeting of the Cognitive Science Society, Hillsdale, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl de Marcken</author>
</authors>
<title>Parsing with stochastic, feature-based RTNs</title>
<date>1995</date>
<journal>Memo A.I. Memo, MIT Artificial Intelligence Lab</journal>
<location>Cambridge, Massachusetts</location>
<marker>de Marcken, 1995</marker>
<rawString>Carl de Marcken. 1995. Parsing with stochastic, feature-based RTNs. Memo A.I. Memo, MIT Artificial Intelligence Lab., Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Della-Pietra</author>
<author>V Della-Pietra</author>
<author>J Gillett</author>
<author>J Lafferty</author>
<author>H Printz</author>
<author>L Ureg</author>
</authors>
<title>Inference and estimation of a long-range trigram model</title>
<date>1994</date>
<booktitle>In International Colloquium on Grammatical Inference</booktitle>
<pages>78--92</pages>
<location>Alicante, Spain</location>
<marker>Della-Pietra, Della-Pietra, Gillett, Lafferty, Printz, Ureg, 1994</marker>
<rawString>S. Della-Pietra, V. Della-Pietra, J. Gillett, J. Lafferty, H. Printz, and L. Ureg. 1994. Inference and estimation of a long-range trigram model. In International Colloquium on Grammatical Inference, pages 78-92, Alicante, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mark Ellison</author>
</authors>
<title>The Machine Learning of Phonological Structure</title>
<date>1992</date>
<tech>Ph.D. thesis</tech>
<institution>University of Western Australia</institution>
<contexts>
<context position="8948" citStr="Ellison, 1992">to those discussed here have been used to acquiring words and morphemes from sequences of characters (Olivier, 1968; Wolff, 1982; Brent, 1993; Cartwright and Brent, 1994) and syllables from phonemes (Ellison, 1992), among other language applications. 16 explore ways of fixing them. Let us look again at (A), reproduced below, and center discussion on an extended stochastic context-free grammar model in which a b</context>
</contexts>
<marker>Ellison, 1992</marker>
<rawString>T. Mark Ellison. 1992. The Machine Learning of Phonological Structure. Ph.D. thesis, University of Western Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Daniel Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Grammatical trigrams: A probabalistic model of link grammar</title>
<date>1992</date>
<tech>Technical Report CMU-CS-92-181</tech>
<institution>Carnegie Mellon University</institution>
<location>Pittsburgh, Pennsylvania</location>
<contexts>
<context position="30196" citStr="Lafferty et al., 1992"> We have implemented a statistical parser and training mechanism based on the above notions, but results are too preliminary to include here. Stochastic link-grammar based models have been discussed (Lafferty et al., 1992) but the only test results we have seen (Della-Pietra et al., 1994) assume a very restricted subset of the model and do not explore the &amp;quot;phrase structures&amp;quot; that result from training on English text. R</context>
</contexts>
<marker>Lafferty, Sleator, Temperley, 1992</marker>
<rawString>John Lafferty, Daniel Sleator, and Davy Temperley. 1992. Grammatical trigrams: A probabalistic model of link grammar. Technical Report CMU-CS-92-181, Carnegie Mellon University, Pittsburgh, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lan i</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the inside-outside algorithm</title>
<date>1990</date>
<journal>Computer Speech and Language</journal>
<pages>4--35</pages>
<contexts>
<context position="13763" citStr="i and Young, 1990">y starting with an exhaustive set of stochastic context-free rules of a certain form, and estimate probabilities for these rules from a test corpus. This is the same general procedure as used by (Lan i and Young, 1990; Briscoe and Waegner, 1992; Pereira and Schabes, 1992) and others. For parts-of-speech Y and Z, the rules we include in our base grammar are S ZP	ZP ZP YP ZP YP ZP ZP Z YP ZP YP Z ZP Z where S is the</context>
</contexts>
<marker>i, Young, 1990</marker>
<rawString>K. Lan i and S. J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Parsing a natural language using mutual information statistics</title>
<date>1990</date>
<booktitle>In Proc. of the American Association for Artificial Intelligence</booktitle>
<pages>984--989</pages>
<contexts>
<context position="5555" citStr="Magerman and Marcus, 1990">s has been generated from a phrase-structure grammar, it suggests that we can recover internal structure by grouping subsequences of words with high mutual information. This is the approach taken by (Magerman and Marcus, 1990) for parsing sentences, who use mutual information rather than a grammar to reconstruct phrase-structure. The hope is that by searching for a phrase-structure or phrase-structure grammar that maximize</context>
<context position="6395" citStr="Magerman and Marcus, 1990">ic context free grammar trained on part-of-speech sequences from English text can have an entropy as low or lower than another but bracket the text much more poorly (tested on hand-annotations). And (Magerman and Marcus, 1990) provide evidence that grouping sub-sequences of events with high mutual information is not always a good heuristic; they must include in their parsing algorithm a list of event sequences (such as nou</context>
<context position="7758" citStr="Magerman and Marcus, 1990">occur together not because they are part of a common word, but because English syntax and semantics places these two morphemes sideby-side. At a syntactic level, this is exactly why the algorithm of (Magerman and Marcus, 1990) has problems: English places prepositions after nouns not because they are in the same phrase, but because prepositional phrases often adjoin to noun phrases. Any greedy algorithm (such as (Magerman </context>
</contexts>
<marker>Magerman, Marcus, 1990</marker>
<rawString>David M. Magerman and Mitchell P. Marcus. 1990. Parsing a natural language using mutual information statistics. In Proc. of the American Association for Artificial Intelligence, pages 984-989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
</authors>
<title>Very large annotated databse of American English</title>
<date>1991</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop</booktitle>
<contexts>
<context position="1191" citStr="Marcus, 1991">mmars (SCFGs), tend to produce grammars that structure text in ways contrary to our linguistic intuitions. One effective way around this problem is to use hand-structured text like the Penn Treebank (Marcus, 1991) to train the learner: (Pereira and Schabes, 1992) demonstrate that the inside-outside algorithm can learn grammars effectively given such constraint; from a bracketed corpus (Brill, 1993) successfull</context>
</contexts>
<marker>Marcus, 1991</marker>
<rawString>Mitchell Marcus. 1991. Very large annotated databse of American English. In Proceedings of the DARPA Speech and Natural Language Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A MelCuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice</title>
<date>1988</date>
<publisher>State University of New York Press</publisher>
<marker>MelCuk, 1988</marker>
<rawString>I. A. MelCuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Cort Olivier</author>
</authors>
<title>Stochastic Grammars and Language Acquisition Mechanisms</title>
<date>1968</date>
<tech>Ph.D. thesis</tech>
<institution>Harvard University</institution>
<location>Cambridge, Massachusetts</location>
<contexts>
<context position="6814" citStr="Olivier, 1968">position) that should not be grouped together in a single phrase, in order to prevent their method from mis-bracketing. To understand why, we can look at an example from a slightly different domain. (Olivier, 1968) seeks to acquire a lexicon from unsegmented (spaceless) character sequences by treating each word as a stochastic context free rule mapping a common nonterminal (call it W) to a sequence of letters; </context>
<context position="8849" citStr="Olivier, 1968">ile this paper concentrates on the acquisition of syntax, similar or identical statistical models to those discussed here have been used to acquiring words and morphemes from sequences of characters (Olivier, 1968; Wolff, 1982; Brent, 1993; Cartwright and Brent, 1994) and syllables from phonemes (Ellison, 1992), among other language applications. 16 explore ways of fixing them. Let us look again at (A), reprod</context>
</contexts>
<marker>Olivier, 1968</marker>
<rawString>Donald Cort Olivier. 1968. Stochastic Grammars and Language Acquisition Mechanisms. Ph.D. thesis, Harvard University, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>Inside-outside reestimation from partially bracketed corpora</title>
<date>1992</date>
<booktitle>In Proc. 29th Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>128--135</pages>
<location>Berkeley, California</location>
<contexts>
<context position="1241" citStr="Pereira and Schabes, 1992">rs that structure text in ways contrary to our linguistic intuitions. One effective way around this problem is to use hand-structured text like the Penn Treebank (Marcus, 1991) to train the learner: (Pereira and Schabes, 1992) demonstrate that the inside-outside algorithm can learn grammars effectively given such constraint; from a bracketed corpus (Brill, 1993) successfully learns rules that iteratively transform a defaul</context>
<context position="6119" citStr="Pereira and Schabes, 1992">uantitative evidence that simple techniques for estimating phrase-structure grammars by minimizing entropy do not lead to the desired grammars (grammars that agree with structure (A), for instance). (Pereira and Schabes, 1992) explore this topic, demonstrating that a stochastic context free grammar trained on part-of-speech sequences from English text can have an entropy as low or lower than another but bracket the text mu</context>
<context position="13818" citStr="Pereira and Schabes, 1992">c context-free rules of a certain form, and estimate probabilities for these rules from a test corpus. This is the same general procedure as used by (Lan i and Young, 1990; Briscoe and Waegner, 1992; Pereira and Schabes, 1992) and others. For parts-of-speech Y and Z, the rules we include in our base grammar are S ZP	ZP ZP YP ZP YP ZP ZP Z YP ZP YP Z ZP Z where S is the root nonterminal. As is ususal with stochastic context</context>
<context position="23730" citStr="Pereira and Schabes, 1992"> feature out. 4.3. Testing on the Penn Treebank To test whether head-driven language models do indeed converge to linguistically-motivated grammars better than SCFGs, we replicated the experiment of (Pereira and Schabes, 1992) on the ATIS section of the Penn Treebank. The 48 parts-of-speech in the Treebank were collapsed to 25, resulting in 2550 grammar rules. Word head features were created by assigning numbers a common f</context>
<context position="24723" citStr="Pereira and Schabes, 1992">ere skipped. We ran four experiments, training a grammar with and without bracketing and with and without use of features. Without features, we are essentially replicating the two experiments run by (Pereira and Schabes, 1992), except that they use a different set of initial rules (all 4095 CNF grammar rules over 15 nonterminals and the 48 Treebank terminal categories). Every tenth sentence of the 1129 sentences in the ATI</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Fernando Pereira and Yves Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In Proc. 29th Annual Meeting of the Association for Computational Linguistics,. pages 128-135, Berkeley, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D K Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing english with a link grammar</title>
<date>1991</date>
<tech>Technical Report CMU-CS-91-196</tech>
<institution>Carnegie Mellon University</institution>
<location>Pittsburgh, Pennsylvania</location>
<contexts>
<context position="28170" citStr="Sleator and Temperley, 1991"> is essentially incapable of finding an optimal grammar without bracketing help. We now suggest that a representation that explicitly represents relations between phrase heads, such as link grammars (Sleator and Temperley, 1991), is far more amenable to language acquisition problems. Let us look one final time at the sequence V P N. There are only three words here, and therefore three heads. Assuming a head-driven bigram mod</context>
</contexts>
<marker>Sleator, Temperley, 1991</marker>
<rawString>Daniel D. K. Sleator and Davy Temperley. 1991. Parsing english with a link grammar. Technical Report CMU-CS-91-196, Carnegie Mellon University, Pittsburgh, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Bayesian Learning of Probabalistic Language Models</title>
<date>1994</date>
<tech>Ph.D. thesis</tech>
<institution>University of California at Berkeley</institution>
<location>Berkeley, CA</location>
<contexts>
<context position="8040" citStr="Stolcke, 1994">hey are in the same phrase, but because prepositional phrases often adjoin to noun phrases. Any greedy algorithm (such as (Magerman and Marcus, 1990) and the context-free grammar induction method of (Stolcke, 1994)) that builds phrases by grouping events with high mutual information will consequently fail to derive linguistically-plausible phrase structure in many situations. 3. INCORPORATING HEADEDNESS INTO LA</context>
</contexts>
<marker>Stolcke, 1994</marker>
<rawString>Andreas Stolcke. 1994. Bayesian Learning of Probabalistic Language Models. Ph.D. thesis, University of California at Berkeley, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gerald Wolff</author>
</authors>
<title>Language acquisition, data compression and generalization</title>
<date>1982</date>
<journal>Language and Communication</journal>
<pages>2--1</pages>
<contexts>
<context position="8862" citStr="Wolff, 1982">concentrates on the acquisition of syntax, similar or identical statistical models to those discussed here have been used to acquiring words and morphemes from sequences of characters (Olivier, 1968; Wolff, 1982; Brent, 1993; Cartwright and Brent, 1994) and syllables from phonemes (Ellison, 1992), among other language applications. 16 explore ways of fixing them. Let us look again at (A), reproduced below, a</context>
</contexts>
<marker>Wolff, 1982</marker>
<rawString>J. Gerald Wolff. 1982. Language acquisition, data compression and generalization. Language and Communication, 2(1):57-89.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>